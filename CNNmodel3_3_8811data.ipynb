{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import pkuseg\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/jupyter/mengxuan\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_info(label, title):\n",
    "    if not isinstance(label, np.ndarray):\n",
    "        label = np.array(label)\n",
    "    print('==='+title+'===')\n",
    "    try:\n",
    "        print('===data info===\\nnumber of record: %s\\npositive: %s\\nnegative: %s\\nneutral: %s\\n' %(len(label), sum(label=='positive'), sum(label=='negative'), sum(label=='neutral')))\n",
    "    except: \n",
    "        try:\n",
    "            print('===data info===\\nnumber of record: %s\\npositive: %s\\nnegative: %s\\nneutral: %s\\n' %(len(label), sum(label==1), sum(label==2), sum(label==0)))\n",
    "        except: print('data info not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(directory, label_col='Polarity', demo = False, exclude_long = False, max_length = 500):\n",
    "    data = pd.read_excel(open(directory, 'rb'))\n",
    "    data = data.dropna(subset=[label_col])\n",
    "    if exclude_long:\n",
    "        data = data[data['Content'].map(len) < max_length]\n",
    "    content = np.array(data['Content'])\n",
    "    label = np.array(data[label_col])\n",
    "    assert content.shape == label.shape\n",
    "    show_info(label, 'original data')\n",
    "    if demo:\n",
    "        print(data[['Content', label_col]].head())\n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(content):\n",
    "    seg = pkuseg.pkuseg()                                  \n",
    "    tokens = [seg.cut(record) for record in content]\n",
    "    print('finish tokenizing all data\\n')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_model(content, model='/home/mengxuan/honda_data/ctb8', exclude_non_chinese=True):\n",
    "    print('using ctb8 tokenizer')\n",
    "    seg = pkuseg.pkuseg(model_name=model, user_dict=None)\n",
    "    if exclude_non_chinese:\n",
    "        tokens = [[t for t in seg.cut(record) if re.findall(u'[\\u4e00-\\u9fff]+', t)] for record in content]\n",
    "    else:\n",
    "        tokens = [seg.cut(record) for record in content]\n",
    "    print('finish tokenizing all data\\n')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def len_control(tokens, label, threshold=300):\n",
    "    return zip(*[(tokens[i], label[i]) for i in range(len(tokens)) if len(tokens[i])<threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#c1,_ = read_data(dataDir[0])\n",
    "#c2,_ = read_data(dataDir[1])\n",
    "#c = np.concatenate((c1, c2))\n",
    "#c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t = tokenize_with_model(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t = len_control(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a = np.array([len(t[i]) for i in range(len(t))])\n",
    "\n",
    "#%matplotlib inline\n",
    "#x = np.random.normal(size = 1000)\n",
    "#plt.hist(a, normed=True, bins=60)\n",
    "#plt.xlim(0, 1000)  \n",
    "#plt.ylabel('Probability');\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sum(a>800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trained_word2vec(directory):\n",
    "    with open(directory, encoding='utf8') as infile:\n",
    "        content = infile.readlines()\n",
    "    \n",
    "    # metadata: number of words in the file / the dimension size\n",
    "    meta = content[0].split(' ')\n",
    "    print('===pre-trained word vec info===\\n','number of words:', meta[0], 'dimension:', meta[1])\n",
    "    \n",
    "    word2vec = [vec.split(' ')[:-1] for vec in content[1:]]\n",
    "    assert len(word2vec[0][1:]) == int(meta[1])\n",
    "    \n",
    "    word2vec_dic = {word2vec[i][0]:np.array([float(n) for n in word2vec[i][1:]], dtype = 'float32') for i in range(len(word2vec))}\n",
    "    print('word vec model is ready\\n')\n",
    "    return word2vec_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_len(tokens):\n",
    "    return max([len(sent) for sent in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def representation(contentTokens, word2vecDict, recordLen, vecDim=300):\n",
    "    print('preparing for sentence matrix representation\\n')\n",
    "    doc = np.zeros((len(contentTokens), recordLen, vecDim), dtype = 'float32')\n",
    "    for i in range(len(contentTokens)):\n",
    "        for j in range(len(contentTokens[i])):\n",
    "            try:\n",
    "                doc[i,j,:] += word2vecDict[contentTokens[i][j]]\n",
    "            except: pass\n",
    "    print('document representation is ready')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def representation_not_sparse(contentTokens, word2vecDict, recordLen, vecDim=300):\n",
    "    print('preparing for sentence matrix representation\\n')\n",
    "    doc = np.zeros((len(contentTokens), recordLen, vecDim), dtype = 'float32')\n",
    "    for i in range(len(contentTokens)):\n",
    "        k = 0\n",
    "        for j in range(len(contentTokens[i])):\n",
    "            try:\n",
    "                doc[i][k] += word2vecDict[contentTokens[i][j]]\n",
    "            except: pass\n",
    "            else: k += 1\n",
    "        #print(doc.shape)\n",
    "    print('document representation is ready')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_adapt(label, labelDict):\n",
    "    print('preparing for labels')\n",
    "    return np.array([labelDict[k] for k in label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dev_split(X, y, ratio=[0.9, 0.1], Shuffle=True):\n",
    "    if sum(ratio) != 1:\n",
    "        raise ValueError('Invalid train/dev split ratio')\n",
    "    if Shuffle:\n",
    "        X, y = shuffle(X, y, random_state=0)\n",
    "    split = int(len(X) * ratio[0])\n",
    "    X_train, X_dev = X[:split], X[split:] \n",
    "    y_train, y_dev = y[:split], y[split:]\n",
    "\n",
    "    return X_train, X_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(dataDir, word2vecDir, labelDict):\n",
    "    content1, label1 = read_data(dataDir[0])\n",
    "    #content2, label2 = read_data(dataDir[1])\n",
    "    tokens1, label1 = len_control(tokenize_with_model(content1), label1)\n",
    "    #tokens2, label2 = len_control(tokenize_with_model(content2), label2)\n",
    "    \n",
    "    weibo_model = trained_word2vec(word2vecDir)\n",
    "    \n",
    "    #width = max(max_len(tokens1), max_len(tokens2))\n",
    "    width = max_len(tokens1)\n",
    "    doc2vec1 = representation_not_sparse(tokens1, weibo_model, width)\n",
    "    #doc2vec2 = representation(tokens2, weibo_model, width)\n",
    "    \n",
    "    labelDigit1 = label_adapt(label1, labelDict)\n",
    "    #labelDigit2 = label_adapt(label2, labelDict)\n",
    "    #assert doc2vec.shape[0] == labelDigit.shape[0]\n",
    "    \n",
    "    X_train, X_dev, y_train, y_dev = train_dev_split(doc2vec1, labelDigit1)\n",
    "    #X_train = np.concatenate((X_train1, doc2vec2))\n",
    "    #y_train = np.concatenate((y_train1, labelDigit2))\n",
    "    \n",
    "    print('data is ready for classifier')\n",
    "    print('training set shape:', X_train.shape, 'training labels:', y_train.shape)\n",
    "    show_info(y_train, 'training')\n",
    "    print('dev set shape:', X_dev.shape, 'dev labels:', y_dev.shape)\n",
    "    show_info(y_dev, 'dev')\n",
    "    return X_train, X_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 108 holdout\n",
    "dataDir = ['/home/mengxuan/honda_data/2019-02-04_CH_Honda_MasterTraining8811.xlsx',\n",
    "           '/home/mengxuan/honda_data/Chinese_Legacy_Training_data.xlsx',\n",
    "          '/home/mengxuan/honda_data/CH_Honda_HoldOut_108.xlsx']\n",
    "\n",
    "# 515 holdout\n",
    "#dataDir = ['/home/mengxuan/honda_data/2019-02-04_CH_Honda_MasterTraining8811.xlsx',\n",
    "#           '/home/mengxuan/honda_data/NLPCC Chinese Sentiment.xlsx',\n",
    "#          '/home/mengxuan/honda_data/2019-01-08_MasterHoldouts515.xlsx']\n",
    "\n",
    "# 821 holdout\n",
    "#dataDir = ['/home/mengxuan/honda_data/2019-02-04_CH_Honda_MasterTraining8811.xlsx',\n",
    "#           '/home/mengxuan/honda_data/NLPCC Chinese Sentiment.xlsx',\n",
    "#          '/home/mengxuan/honda_data/2019-01-08_MasterHoldout821.xlsx']\n",
    "word2vecDir = '/home/mengxuan/honda_data/cgns.weibo.word'\n",
    "\n",
    "# local drive\n",
    "#dataDir = ['C:\\\\Users\\\\Mengxuan Zhao\\\\Google Drive\\\\Clients & Partners\\\\Honda\\\\POC Data\\\\Chinese\\\\TrainingSets\\\\2019-02-04_CH_Honda_MasterTraining8811.xlsx',\n",
    "#           'C:\\\\Users\\\\Mengxuan Zhao\\\\Google Drive\\\\Clients & Partners\\\\Honda\\\\POC Data\\\\Chinese\\\\HoldOuts\\\\CH_Honda_HoldOut_108.xlsx']\n",
    "#word2vecDir = 'sgns.weibo.word'\n",
    "labelDict = {'positive':1, 'negative':2, 'neutral':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===original data===\n",
      "===data info===\n",
      "number of record: 8809\n",
      "positive: 3411\n",
      "negative: 1137\n",
      "neutral: 4261\n",
      "\n",
      "using ctb8 tokenizer\n",
      "finish tokenizing all data\n",
      "\n",
      "===pre-trained word vec info===\n",
      " number of words: 195202 dimension: 300\n",
      "\n",
      "word vec model is ready\n",
      "\n",
      "preparing for sentence matrix representation\n",
      "\n",
      "document representation is ready\n",
      "preparing for labels\n",
      "data is ready for classifier\n",
      "training set shape: (7928, 234, 300) training labels: (7928,)\n",
      "===training===\n",
      "===data info===\n",
      "number of record: 7928\n",
      "positive: 3078\n",
      "negative: 1022\n",
      "neutral: 3828\n",
      "\n",
      "dev set shape: (881, 234, 300) dev labels: (881,)\n",
      "===dev===\n",
      "===data info===\n",
      "number of record: 881\n",
      "positive: 333\n",
      "negative: 115\n",
      "neutral: 433\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyter/venv37/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, X_dev,y_train, y_dev = main(dataDir, word2vecDir, labelDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a = np.zeros((1,300))\n",
    "#b = np.random.rand(500, 300)\n",
    "\n",
    "#for i in range(b.shape[0]):\n",
    "#    a = np.concatenate((a, b[i].reshape(1,300)))\n",
    "#a[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train).float().view(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "X_dev_tensor = torch.from_numpy(X_dev).float().view(X_dev.shape[0], 1, X_dev.shape[1], X_dev.shape[2])\n",
    "#X_test_tensor = torch.from_numpy(X_test).float().view(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "y_dev_tensor = torch.from_numpy(y_dev).long()\n",
    "#y_test_tensor = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(data, batch_size):\n",
    "    #assert len(data)%batch_size == 0\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        if len(data) - i > batch_size:\n",
    "            yield data[i:i+batch_size]\n",
    "        else: yield data[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "X_train_b = tuple(make_batch(X_train_tensor, batch_size))\n",
    "X_dev_b = tuple(make_batch(X_dev_tensor, batch_size))\n",
    "#X_test_b = tuple(make_batch(X_test_tensor, batch_size))\n",
    "y_train_b = tuple(make_batch(y_train_tensor, batch_size))\n",
    "y_dev_b = tuple(make_batch(y_dev_tensor, batch_size))\n",
    "#y_test_b = tuple(make_batch(y_test_tensor, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convfilter2 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=(2,300), stride=1), \n",
    "            nn.Tanh())\n",
    "            #nn.MaxPool2d(kernel_size=(256,1)),\n",
    "            #nn.Dropout(p=0.5))\n",
    "        self.convfilter3 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=(3,300), stride=1), \n",
    "            nn.Tanh())\n",
    "            #nn.MaxPool2d(kernel_size=(255,1)),\n",
    "            #nn.Dropout(p=0.5))\n",
    "        self.convfilter4 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=(4,300), stride=1), \n",
    "            nn.Tanh())\n",
    "            #nn.MaxPool2d(kernel_size=(254,1)),\n",
    "            #nn.Dropout(p=0.5))\n",
    "        self.convfilter5 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=(5,300), stride=1), \n",
    "            nn.Tanh())\n",
    "            #nn.MaxPool2d(kernel_size=(253,1)),\n",
    "            #nn.Dropout(p=0.5))\n",
    "        self.linearlayer1 = nn.Sequential(\n",
    "            nn.Linear(400, 100),\n",
    "            nn.Tanh())\n",
    "        self.linearlayer2 = nn.Sequential(\n",
    "            nn.Linear(100, 3))\n",
    "        self.drop_out_conv = nn.Dropout(p=0.4)\n",
    "        self.drop_out = nn.Dropout(p=0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        map2 = self.convfilter2(x)\n",
    "        #print(map2.shape)\n",
    "        map2 = kmax_pooling(map2, 2, 10)\n",
    "        #print(map2.shape)\n",
    "        map2 = map2.reshape(map2.size(0), -1)\n",
    "        #print(map2.shape)\n",
    "        map2 = self.drop_out_conv(map2)\n",
    "        \n",
    "        map3 = self.convfilter3(x)\n",
    "        #print(map3.shape)\n",
    "        map3 = kmax_pooling(map3, 2, 10)\n",
    "        #print(map3.shape)\n",
    "        map3 = map3.reshape(map3.size(0), -1)\n",
    "        #print(map3.shape)\n",
    "        map3 = self.drop_out_conv(map3)\n",
    "        \n",
    "        map4 = self.convfilter4(x)\n",
    "        map4 = kmax_pooling(map4, 2, 10)\n",
    "        map4 = map4.reshape(map4.size(0), -1)\n",
    "        map4 = self.drop_out_conv(map4)\n",
    "        \n",
    "        map5 = self.convfilter5(x)\n",
    "        map5 = kmax_pooling(map5, 2, 10)\n",
    "        map5 = map5.reshape(map5.size(0), -1)\n",
    "        map5 = self.drop_out_conv(map5)\n",
    "        \n",
    "        out = torch.cat((map2, map3),1)\n",
    "        out = torch.cat((out, map4),1)\n",
    "        \n",
    "        out = torch.cat((out, map5),1)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        \n",
    "        out = self.linearlayer1(out)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.linearlayer2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "if gpu:\n",
    "    print('working on GPU')\n",
    "\n",
    "model = CNN()\n",
    "if gpu:\n",
    "    model.cuda()\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.004\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 155.52579540014267 Accuracy for epoch 10: 0.4989909182643794\n",
      "Epoch 10 done\n",
      "training set precision: 0.3055555555555555\n",
      "training set recall: 0.3283208020050125\n",
      "training set F1 score: 0.3086680761099366\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyter/venv37/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/jupyter/venv37/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Accuracy: 0.52894438138479\n",
      "development set precision: 0.3943292993925905\n",
      "development set recall: 0.37283935205413266\n",
      "development set F1 score: 0.31668055148438984\n",
      "========\n",
      "Cost: 150.52234250307083 Accuracy for epoch 20: 0.5504540867810293\n",
      "Epoch 20 done\n",
      "training set precision: 0.38095238095238093\n",
      "training set recall: 0.406015037593985\n",
      "training set F1 score: 0.39285714285714285\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5402951191827469\n",
      "development set precision: 0.3634638658367912\n",
      "development set recall: 0.3974135798616168\n",
      "development set F1 score: 0.3681213353206625\n",
      "========\n",
      "Cost: 146.7323083281517 Accuracy for epoch 30: 0.5678607467204844\n",
      "Epoch 30 done\n",
      "training set precision: 0.4090909090909091\n",
      "training set recall: 0.42355889724310775\n",
      "training set F1 score: 0.4140087554721701\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5584562996594779\n",
      "development set precision: 0.37977119965936296\n",
      "development set recall: 0.4115801713954139\n",
      "development set F1 score: 0.382555429067057\n",
      "========\n",
      "Cost: 142.6153183579445 Accuracy for epoch 40: 0.5848890010090817\n",
      "Epoch 40 done\n",
      "training set precision: 0.4090909090909091\n",
      "training set recall: 0.42355889724310775\n",
      "training set F1 score: 0.4140087554721701\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5629965947786606\n",
      "development set precision: 0.384406341698419\n",
      "development set recall: 0.4148906412185858\n",
      "development set F1 score: 0.3858509053086004\n",
      "========\n",
      "Cost: 137.94196999073029 Accuracy for epoch 50: 0.6064581231079718\n",
      "Epoch 50 done\n",
      "training set precision: 0.44637681159420284\n",
      "training set recall: 0.44110275689223055\n",
      "training set F1 score: 0.4365079365079365\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5777525539160046\n",
      "development set precision: 0.39703492392807743\n",
      "development set recall: 0.4272101200507667\n",
      "development set F1 score: 0.3988870523773804\n",
      "========\n",
      "Cost: 132.16499161720276 Accuracy for epoch 60: 0.6212159434914228\n",
      "Epoch 60 done\n",
      "training set precision: 0.5866666666666667\n",
      "training set recall: 0.4761904761904762\n",
      "training set F1 score: 0.48787878787878786\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5720771850170261\n",
      "development set precision: 0.3924206968278731\n",
      "development set recall: 0.4226674711663165\n",
      "development set F1 score: 0.3942593621730186\n",
      "========\n",
      "Cost: 125.45413666963577 Accuracy for epoch 70: 0.6455600403632694\n",
      "Epoch 70 done\n",
      "training set precision: 0.5866666666666667\n",
      "training set recall: 0.4761904761904762\n",
      "training set F1 score: 0.48787878787878786\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5686719636776391\n",
      "development set precision: 0.55729015648751\n",
      "development set recall: 0.4217931959472275\n",
      "development set F1 score: 0.39664126797034593\n",
      "========\n",
      "Cost: 117.97077006101608 Accuracy for epoch 80: 0.680247225025227\n",
      "Epoch 80 done\n",
      "training set precision: 0.8412698412698413\n",
      "training set recall: 0.8395989974937343\n",
      "training set F1 score: 0.8090909090909092\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5947786606129398\n",
      "development set precision: 0.6409602845287493\n",
      "development set recall: 0.454583270890129\n",
      "development set F1 score: 0.4454555478708933\n",
      "========\n",
      "Cost: 109.20826137065887 Accuracy for epoch 90: 0.7072401614530777\n",
      "Epoch 90 done\n",
      "training set precision: 0.8555555555555555\n",
      "training set recall: 0.8872180451127819\n",
      "training set F1 score: 0.8521367521367521\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5925085130533485\n",
      "development set precision: 0.5978358723852929\n",
      "development set recall: 0.46155853614433795\n",
      "development set F1 score: 0.4605724713063451\n",
      "========\n",
      "Cost: 99.14206653833389 Accuracy for epoch 100: 0.7454591321897074\n",
      "Epoch 100 done\n",
      "training set precision: 0.9682539682539683\n",
      "training set recall: 0.9047619047619048\n",
      "training set F1 score: 0.9277777777777777\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5936435868331441\n",
      "development set precision: 0.6012208924657602\n",
      "development set recall: 0.47510072578854584\n",
      "development set F1 score: 0.48360478146799624\n",
      "========\n",
      "Cost: 90.08498844504356 Accuracy for epoch 110: 0.7692986881937437\n",
      "Epoch 110 done\n",
      "training set precision: 0.9545454545454546\n",
      "training set recall: 0.8571428571428571\n",
      "training set F1 score: 0.8847006651884701\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5788876276958003\n",
      "development set precision: 0.5630919910736424\n",
      "development set recall: 0.47730678765923273\n",
      "development set F1 score: 0.4890880007025337\n",
      "========\n",
      "Cost: 88.39372104406357 Accuracy for epoch 120: 0.7660191725529768\n",
      "Epoch 120 done\n",
      "training set precision: 0.9420289855072465\n",
      "training set recall: 0.8095238095238096\n",
      "training set F1 score: 0.834920634920635\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5664018161180476\n",
      "development set precision: 0.5381547645109972\n",
      "development set recall: 0.4732792436979605\n",
      "development set F1 score: 0.4840104766361229\n",
      "========\n",
      "Cost: 79.86377282440662 Accuracy for epoch 130: 0.7936427850655903\n",
      "Epoch 130 done\n",
      "training set precision: 0.9833333333333334\n",
      "training set recall: 0.9523809523809524\n",
      "training set F1 score: 0.9658119658119658\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5720771850170261\n",
      "development set precision: 0.5551956422924165\n",
      "development set recall: 0.4601079842770775\n",
      "development set F1 score: 0.4671385338052005\n",
      "========\n",
      "Cost: 63.7888950407505 Accuracy for epoch 140: 0.8531786074672049\n",
      "Epoch 140 done\n",
      "training set precision: 0.9833333333333334\n",
      "training set recall: 0.9523809523809524\n",
      "training set F1 score: 0.9658119658119658\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5561861520998865\n",
      "development set precision: 0.5167072248953782\n",
      "development set recall: 0.4471536100816149\n",
      "development set F1 score: 0.45254749965817553\n",
      "========\n",
      "Cost: 58.95291368663311 Accuracy for epoch 150: 0.8693239152371343\n",
      "Epoch 150 done\n",
      "training set precision: 0.9833333333333334\n",
      "training set recall: 0.9523809523809524\n",
      "training set F1 score: 0.9658119658119658\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5595913734392736\n",
      "development set precision: 0.5146608633531334\n",
      "development set recall: 0.45816104285830156\n",
      "development set F1 score: 0.4659608657738843\n",
      "========\n",
      "Cost: 42.89667998254299 Accuracy for epoch 160: 0.9239404641775983\n",
      "Epoch 160 done\n",
      "training set precision: 0.9833333333333334\n",
      "training set recall: 0.9523809523809524\n",
      "training set F1 score: 0.9658119658119658\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5584562996594779\n",
      "development set precision: 0.5261830352195404\n",
      "development set recall: 0.4654437749326794\n",
      "development set F1 score: 0.476382179711686\n",
      "========\n",
      "Cost: 65.80100990831852 Accuracy for epoch 170: 0.8577194752774975\n",
      "Epoch 170 done\n",
      "training set precision: 1.0\n",
      "training set recall: 1.0\n",
      "training set F1 score: 1.0\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5561861520998865\n",
      "development set precision: 0.5147611583063646\n",
      "development set recall: 0.4699628838598615\n",
      "development set F1 score: 0.4802155961034466\n",
      "========\n",
      "Cost: 25.34328681975603 Accuracy for epoch 180: 0.972502522704339\n",
      "Epoch 180 done\n",
      "training set precision: 1.0\n",
      "training set recall: 1.0\n",
      "training set F1 score: 1.0\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5505107832009081\n",
      "development set precision: 0.5101330506867966\n",
      "development set recall: 0.4734727899905127\n",
      "development set F1 score: 0.4837534372686454\n",
      "========\n",
      "Cost: 20.21725619956851 Accuracy for epoch 190: 0.9818365287588294\n",
      "Epoch 190 done\n",
      "training set precision: 1.0\n",
      "training set recall: 1.0\n",
      "training set F1 score: 1.0\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5357548240635641\n",
      "development set precision: 0.49849117037029583\n",
      "development set recall: 0.46097025833947214\n",
      "development set F1 score: 0.47064844819260127\n",
      "========\n",
      "Cost: 15.526555132120848 Accuracy for epoch 200: 0.9881432896064581\n",
      "Epoch 200 done\n",
      "training set precision: 1.0\n",
      "training set recall: 1.0\n",
      "training set F1 score: 1.0\n",
      "\n",
      "\n",
      "Dev Accuracy: 0.5346197502837684\n",
      "development set precision: 0.49355600539811073\n",
      "development set recall: 0.46112514763985796\n",
      "development set F1 score: 0.47009202062678984\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    correct = 0\n",
    "    loss_sum = 0\n",
    "    for j, (x, y) in enumerate(zip(X_train_b, y_train_b)):\n",
    "        if gpu:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the accuracy\n",
    "        if gpu:\n",
    "        else:\n",
    "            _, pred = outputs.data.max(1)\n",
    "        \n",
    "        #print('output:', outputs.data, 'prediction:', pred)\n",
    "        correct += (pred == y).sum().item()\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "        if (j+1) == (len(X_train_b)) and (i+1)%10 == 0:\n",
    "            print('Cost:',loss_sum,'Accuracy for epoch %s:' %(i+1), correct / X_train.shape[0])\n",
    "            print('Epoch %s done' %(i+1))\n",
    "            precision = precision_score(y, pred, average='macro')\n",
    "            recall = recall_score(y, pred, average='macro')\n",
    "            f1 = f1_score(y, pred, average='macro')\n",
    "            print('training set precision:', precision)\n",
    "            print('training set recall:', recall)\n",
    "            print('training set F1 score:', f1)\n",
    "            print('\\n')\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct_dev = 0\n",
    "                pred_dev = []\n",
    "                for x, y in zip(X_dev_b, y_dev_b):\n",
    "                    if gpu:\n",
    "                        x, y = x.cuda(), y.cuda()\n",
    "                    outputs = model(x)\n",
    "                    if gpu:\n",
    "                        pred = outputs.data.max(1)[1].cuda()\n",
    "                    else:\n",
    "                        _, pred = outputs.data.max(1)\n",
    "                    correct_dev += (pred == y).sum().item()\n",
    "                    pred_dev.append(pred)\n",
    "                pred_dev = list(itertools.chain.from_iterable(pred_dev))\n",
    "                pred_dev = np.array([i.cpu().item() for i in pred_dev])\n",
    "\n",
    "                precision_dev = precision_score(y_dev, pred_dev, average='macro')\n",
    "                recall_dev = recall_score(y_dev, pred_dev, average='macro')\n",
    "                f1_dev = f1_score(y_dev, pred_dev, average='macro')\n",
    "                print('Dev Accuracy:' , correct_dev / X_dev.shape[0])\n",
    "                print('development set precision:', precision_dev)\n",
    "                print('development set recall:', recall_dev)\n",
    "                print('development set F1 score:', f1_dev)\n",
    "                print('========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window size: [2, 3, 4, 5]\n",
    "stride: 1\n",
    "channel: 10\n",
    "dropout: [conv: 0.4, linear: 0.5]\n",
    "learning rate: 0.004\n",
    "epoch: 100\n",
    "k for k-max pooling: 10\n",
    "linear layer: 2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
